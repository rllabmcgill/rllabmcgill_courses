<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.o1;2crg/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link href="index.css" rel="stylesheet" type="text/css" />
<title>Machine Learning (COMP-652 and ECSE-608)</title>

</head>

  
  
  <body>
    <div id="masthead">

      Machine Learning (COMP-652 and ECSE-608)<br>
Fall 2018
      </div>

    <hr/>

    <div id="menu">
      <!--enter your hyperlinks here-->
      <ul>
	<li><a href="index.html"><p>Home</p></a></li>
       <li><a href="syllabus.html"><p>Syllabus</p></a></li>
	<li><a href="lectures.html"><p>Lectures</p></a></li>
        <li><a href="assignments.html"><p>Assignments</p></a></li>
	<li><a href="project.html"><p>Project</p></a></li>
	<li><a href="resources.html"><p>Resources</p></a></li>
	</ul>
      <!--end of hyperlinks-->
      </div>

<div id="mainText">

<center><h2>Lecture Schedule</h2></center>

      <table border='0' cellspacing='0' cellpadding='0'>
	<tr>
	  <td width=8%><b>Date</b></td>
	  <td width=32%><b>Topic</b></td>
	  <td width=52%><b>Materials</b></td>
	  </tr>
<tr><td>Sep. 5</td><td>Introduction and Linear Models.</td>
<td>
<!-- <a href="lectures/lecture-1.pdf">Lecture 1 slides</a> - -->
<!-- Bishop, Sec. 1.1, 3.1, 3.2 (or equivalent) <br><br> -->
If you need to catch up on the math:
<ul>
<li> A brief <a href="http://cs229.stanford.edu/section/cs229-prob.pdf "> probability review</a> from Stanford University;
<li> <a href="http://cs229.stanford.edu/section/cs229-linalg.pdf">Linear algebra and matrix calculus review</a> also from Stanford;</li>
<li> Bishop appendix B,C.</li>
<li> The <a href=https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf> Matrix Cookbook</a> is a collection of matrix identities and relations (very useful for multivariate calculus, e.g. how to take the gradient of a loss w.r.t. a weight vector...)</li>
</ul>
</td></tr>

<tr><td>Sep. 10</td><td>Overfitting, Regularization and Chatbot tutor demo.</td>
<td>

<!-- <a href="lectures/lecture-2.pdf">Lecture 2 slides</a> -
Bishop, Sec. 1.3, 3.1, 3.2, Hastie Sec. 3.4, 7.1-3, 7.10<br><br>
If you need to catch up on the math:
<ul>
<li> <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint"> Lagrange multipliers</a> on Khan academy;</li>
<li> Bishop appendix E</li> -->


</ul>
</td></tr>

<tr><td>Sep. 12</td><td>Bayesian and Probabilistic ML. </td>
<td>
<!-- <a href="lectures/lecture-3.pdf">Lecture 3 slides</a> -
Related lectures and materials <br>
<ul>
<li> Book:Bishop PRML : Section 1.2 (Probability Theory); </li>
<li> Book:Barber BRML : Chapter 1 (Probabilistic Reasoning); </li>
<li> Video  <a href="https://www.youtube.com/watch?v=mgBrXnjF8R4 "> Bayesian Inference</a> from Zoubin Ghahramani;</li>
<li> Book: Bishop PRML: Section 2.3 (The Gaussian Distribution). This is a truly excellent and in-depth discussion! </li>
<li> Book: Bishop PRML: Section 3.3 (Bayesian Linear Regression).</li>
<li> Nando de Freitas has a series of lectures on Bayesian linear regression.</li>
</ul> -->
</td></tr>
	      
	      
	      
<tr><td>Sep. 17</td><td>Parametric Regression.</td>
<td>

<!-- <a href="lectures/lecture-4.pdf">Lecture 4 slides</a> -
Related material:<br>
<ul>
<li> Bishop, Sec. 4.1.1-4.1.3: Linear models for classification</li>
<li> Bishop, Sec. 4.3.2-4.3.4: Logistic regression </li>
<li> Bishop, Sec. 1.6: Information theory (optional for now but will be relevant for future lectures...)</li>
</ul> -->

</td></tr>


<tr><td>Sep. 19</td><td>Non-Parametric Regression and Gaussian Processes</td>
<td>
<!-- <a href="lectures/lecture-5.pdf">Lecture 5 slides</a> -
Related material:<br>
<ul>
<li> Bishop, Sec. 6.1-6.2: Kernels </li>
<li> Bishop, Sec. 7.1: Support Vector Machines </li>
<li> David Sontag's (NYU) slides on SVMs and kernels: lectures
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture3.pdf> 3</a>,
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture4.pdf> 4</a>,
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture5.pdf> 5</a> and
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture6.pdf> 6 </a>
<li> A python <a href=http://nbviewer.jupyter.org/github/rllabmcgill/rllabmcgill.github.io/blob/master/COMP-652/lectures/SVMs.ipynb>
notebook</a> to experiment with regularization in SVMs</li>
</ul> -->
</td></tr>



<tr><td>Sep. 24</td><td>Support Vector Machines</td>
<td>
<!-- <a href="lectures/lecture-6.pdf">Lecture 6 slides</a> -
Related material:<br>
<ul>
<li>Bishop 12.1, 12.3: PCA and Kernel PCA </li>
<li>Bishop 9.1: <i>k</i>-means</li>
<li><a href="https://www.cs.nyu.edu/~roweis/lle/papers/lleintro.pdf">Locally Linear Embeddings</a> (optional)</li>
</ul> -->
</td></tr>

	      
	      
<tr><td>Sep. 26</td><td>Neural Networks</td>
<td>
<!-- <a href="lectures/lecture-7.pdf">Lecture 7 slides</a> -
Related material:<br>
<ul>
<li><a href="http://mlg.eng.cam.ac.uk/zoubin/talks/nips09npb.pdf">Overview of Non-parametric Bayesian Models</a></li>
<li><a href="http://mlss2011.comp.nus.edu.sg/uploads/Site/lect1gp.pdf">Tutorial on Gaussian Processes (why I don't use SVMs)</a></li>
<li><a href="http://videolectures.net/mlss09uk_rasmussen_gp/">Carl Rasmussen's Lecture on Gaussian Processes</a></li>
<li><a href="http://www.gaussianprocess.org/gpml/">Book on Gaussian Processes</a></li>
</ul> -->
</td></tr>

	      
	      
	      
<tr><td>Oct. 1</td><td>Deep Learning I</td><td>
<!-- <a href="lectures/lecture-8.pdf">Lecture 8 slides</a> - Related material:<br>
<ul>

<li> Olivier Bousquet's <a href=http://ml.typepad.com/Talks/pdf2522.pdf>slides from MLSS 2003 </a> </li>
<li> Alexander Rakhlin's <a href=http://www-stat.wharton.upenn.edu/~rakhlin/ml_summer_school.pdf>slides from MLSS 2012 </a>
<li> Mehryar Mohri's lectures <a href=http://www.cs.nyu.edu/~mohri/mls/>slides</a> at NYU </li>
<li>
Books to go further:
<ul>
<li> <a href=http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf>Understanding Machine Learning</a>, Shai Shalev-Shwartz and Shai Ben-David </li>
<li> Foundations of Machine Learning,
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar
</ul>
</li>
</ul> -->



</td></tr>

<tr><td>Oct. 3</td><td>Deep Learning II</td><td>
<!-- <a href="lectures/lecture-9.pdf">Lecture 9 slides</a>  -->
</td></tr>	      
	   

<tr><td>Oct. 10</td><td>Gaussian Mixture Models and Expectation Maximization</td><td>
<!-- <a href="lectures/lecture-10.pdf">Lecture 10 slides</a>  -->
</td></tr>	      
	   

<tr><td>Oct. 15</td><td>Dimensionality Reduction</td>
<td>
<!-- <a href="lectures/lecture-11.pdf">Lecture 11 slides</a> -
Related material:<br>
<ul>
<li> Bishop, Section 9</li>
<li> Hastie, Section 8.5</li>
</ul> -->
</td></tr>





<tr><td>Oct. 17</td><td>Bandit Algorithms I</td>
<td>
<!-- <a href='lectures/lecture-12.pdf'>Lecture 12 slides</a>  -->
</td>
</tr>



<tr><td>Oct. 22</td><td>Bandit Algorithms II</td>
<td>
<!-- <a href=http://www.cs.upc.edu/~bballe/slides/tutorial-emnlp14.pdf>  Lecture 13 slides</a> (pages 1-81) -  courtesy of B. Balle, A. Quattoni and X. Carrreras<br>
<i> Machine Learning </i> <a href=http://www.lancaster.ac.uk/~deballep/papers/preprint-bclq13.pdf>paper</a> -->
</tr>
	
<tr><td>Oct. 24</td><td>Reinforcement Learning I</td>
<td>
<!-- <a href='lectures/lecture-14.pdf'>Lecture 14 slides</a>  -->
</td>
</tr>
	

<tr><td>Oct. 29</td><td>Reinforcement Learning II</td>
<td>
<!-- <a href='lectures/lecture-15.pdf'>Lecture 15 slides</a>  -->
</td>
</tr>

	
<tr><td>Oct. 31</td><td>Hidden Markov Models</td>
<td>
<!-- <a href='lectures/lecture-16.pdf'>Lecture 16 slides</a>  -->
</td>
</tr>


	      
<tr><td>Nov. 5</td><td>Active Learning and Bayesian Optimization</td>
<td>
<!-- <a href="lectures/lecture-17.pdf">Lecture 17 slides</a> -
Related material:<br>
<ul>
<li><a href="http://www.cs.columbia.edu/~blei/talks/2016_NIPS_VI_tutorial.pdf">Variational Inference : NIPS 2017 Tutorial - courtesy of David Blei, Rajesh Ranganath, Shakir Mohamed</a></li>
<li><a href="https://arxiv.org/abs/1601.00670">Variational Inference : Review for Statisticians</a></li>
</ul> -->
</td></tr>	
	

<tr><td>Nov. 7</td><td>Probabilistic Graphical Models</td>
<td>
<!-- <a href='lectures/lecture-18.pdf'>Lecture 18 slides (Courtesy of Fei Fei-Fei Li, Justin Johnson, Serena Yeung)</a>  -->
</td>
</tr>
	
	
<tr><td>Nov. 9</td><td>Optimization Methods</td><td></td></tr>
	
<tr><td>Nov. 14</td><td>Apprximate Inference </td><td>
<!-- <a href='lectures/lecture-20.pdf'>Lecture 20 slides</a> -->
</td></tr>

<tr><td>Nov. 16</td><td>Variational Inference</td><td></td></tr>

<tr><td>Nov. 21</td><td>Generative Models</td><td></td></tr>

<tr><td>Nov. 26</td><td>No class due to project</td><td></td></tr>
<tr><td>Nov. 28</td><td>No class due to project</td><td></td></tr>

<tr><td>Nov. 30</td><td>Recap</td><td></td></tr>

<tr><td>Dec. 3</td><td>No class: Project report due</td><td></td></tr>

<tr><td>TBD</td><td>Final exam</td><td></td></tr>


<!-- <tr><td>Nov. 21</td><td>In-class midtern exam</td><td>
You are allowed one double-sided &quot;cheat sheet&quot;<br>
<a href="midterm/ml-sample-questions-2015.pdf">Some examples of midterm-style questions with solutions</a><br>
<a href="midterm/ml-mid-2016.pdf">Midterm from 2016</a>
</td></tr>
<tr><td>Nov. 27</td><td>Reinforcement Learning</td><td>
<a href='lectures/lecture-21.pdf'>Lecture 21 slides</a>
</td></tr>
<tr><td>Nov. 29</td><td>More on Reinforcement Learning</td><td></td></tr>
<tr><td>Dec. 4</td><td>no class</td><td></td></tr>
<tr><td>Dec. 6</td><td>no class</td><td></td></tr>
<tr><td>Dec. 11<br> (1-5pm)</td><td>Projects Presentations 1</td><td></td></tr>
<tr><td>Dec. 13<br> (1-5pm)</td><td>Projects Presentations 2</td><td></td></tr> -->


</table>
<p>
&nbsp;
</p>

  </body>
</html>
