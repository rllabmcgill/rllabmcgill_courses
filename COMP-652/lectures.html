<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.o1;2crg/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link href="index.css" rel="stylesheet" type="text/css" />
<title>Machine Learning (COMP-652 and ECSE-608)</title>

</head>

  
  
  <body>
    <div id="masthead">

      Machine Learning (COMP-652 and ECSE-608)<br>
Fall 2017
      </div>

    <hr/>

    <div id="menu">
      <!--enter your hyperlinks here-->
      <ul>
	<li><a href="index.html"><p>Home</p></a></li>
       <li><a href="syllabus.html"><p>Syllabus</p></a></li>
	<li><a href="lectures.html"><p>Lectures</p></a></li>
        <li><a href="assignments.html"><p>Assignments</p></a></li>
	<li><a href="project.html"><p>Project</p></a></li>
	<li><a href="resources.html"><p>Resources</p></a></li>
	</ul>
      <!--end of hyperlinks-->
      </div>

<div id="mainText">

<center><h2>Lecture Schedule</h2></center>

      <table border='0' cellspacing='0' cellpadding='0'>
	<tr>
	  <td width=8%><b>Date</b></td>
	  <td width=32%><b>Topic</b></td>
	  <td width=52%><b>Materials</b></td>
	  </tr>
<tr><td>Sep. 6</td><td>Introduction. Linear Models.</td>
<td>
<a href="lectures/lecture-1.pdf">Lecture 1 slides</a> -
Bishop, Sec. 1.1, 3.1, 3.2 (or equivalent) <br><br>
If you need to catch up on the math:
<ul>
<li> A brief <a href="Materials/prob-review.pdf"> probability review</a> from Stanford University;
<li> <a href="Materials/linalg-review.pdf">Linear algebra and matrix calculus review</a> also from Stanford;</li>
<li> Bishop appendix B,C.</li>
</ul>
</td></tr>

<tr><td>Sep. 11</td><td>More on Linear Models. Overfitting. Regularization.</td>
<td>

If you need to catch up on the math:
<ul>
<li> <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/lagrange-multiplier-example-part-1"> Lagrange multipliers</a> on Khan academy;
</ul>
</td></tr>

<tr><td>Sep. 13</td><td>Bayesian and Maximum Likelihood Fitting. </td><td></td></tr>
<tr><td>Sep. 18</td><td>Logistic Regression. Kernels</td><td></td></tr>
<tr><td>Sep. 20</td><td>Support Vector Machines</td><td></td></tr>
<tr><td>Sep. 25</td><td>Non-Parametric Models and Gaussian Processes</td><td></td></tr>
<tr><td>Sep. 27</td><td>Unsupervised Learning and Dimensionality Reduction</td><td></td></tr>
<tr><td>Oct. 2</td><td>Computational Learning Theory</td><td></td></tr>
<tr><td>Oct. 4</td><td>Probabilistic Graphical Models (PGMs)</td><td></td></tr>
<tr><td>Oct. 11</td><td>Inference in PGMs</td><td></td></tr>
<tr><td>Oct. 16</td><td>Latent Variables Models, Gaussian Mixture Models, Expectation Maximization</td><td></td></tr>
<tr><td>Oct. 18</td><td>Time series / Inference in HMMS</td><td></td></tr>
<tr><td>Oct. 23</td><td>Spectral Learning</td><td></td></tr>
<tr><td>Oct. 25</td><td>Learning Dynamical Systems: Bayesain Updating, Kalman Filters...</td><td></td></tr>
<tr><td>Oct. 30</td><td>Approximate Inference in PGMs</td><td></td></tr>
<tr><td>Nov. 1</td><td>Variational Inference</td><td></td></tr>
<tr><td>Nov. 6</td><td>Semi-supervised learning, Active learning.</td><td></td></tr>
<tr><td>Nov. 8</td><td>Neural Networks and co. (RNNs, CNNs, ...)</td><td></td></tr>
<tr><td>Nov. 13</td><td>Generative Adversarial Networks</td><td></td></tr>
<tr><td>Nov. 15</td><td>Tensor Methods in ML</td><td></td></tr>
<tr><td>Nov. 20</td><td>midterm recap</td><td></td></tr>
<tr><td>Nov. 22</td><td>In-class midtern exam</td><td></td></tr>
<tr><td>Nov. 27</td><td>Reinforcement Learning</td><td></td></tr>
<tr><td>Nov. 29</td><td>More on Reinforcement Learning</td><td></td></tr>
<tr><td>Dec. 4</td><td>TBA</td><td></td></tr>
<tr><td>Dec. 6</td><td>TBA</td><td></td></tr>
<tr><td>Dec. 11</td><td>Projects Presentations 1</td><td></td></tr>
<tr><td>Dec. 13</td><td>Projects Presentations 2</td><td></td></tr>


</table>
<p>
&nbsp;
</p>

  </body>
</html>
