<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.o1;2crg/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link href="index.css" rel="stylesheet" type="text/css" />
<title>Machine Learning (COMP-652 and ECSE-608)</title>

</head>

  
  
  <body>
    <div id="masthead">

      Machine Learning (COMP-652 and ECSE-608)<br>
Fall 2017
      </div>

    <hr/>

    <div id="menu">
      <!--enter your hyperlinks here-->
      <ul>
	<li><a href="index.html"><p>Home</p></a></li>
       <li><a href="syllabus.html"><p>Syllabus</p></a></li>
	<li><a href="lectures.html"><p>Lectures</p></a></li>
        <li><a href="assignments.html"><p>Assignments</p></a></li>
	<li><a href="project.html"><p>Project</p></a></li>
	<li><a href="resources.html"><p>Resources</p></a></li>
	</ul>
      <!--end of hyperlinks-->
      </div>

<div id="mainText">

<center><h2>Lecture Schedule</h2></center>

      <table border='0' cellspacing='0' cellpadding='0'>
	<tr>
	  <td width=8%><b>Date</b></td>
	  <td width=32%><b>Topic</b></td>
	  <td width=52%><b>Materials</b></td>
	  </tr>
<tr><td>Sep. 6</td><td>Introduction. Linear Models.</td>
<td>
<a href="lectures/lecture-1.pdf">Lecture 1 slides</a> -
Bishop, Sec. 1.1, 3.1, 3.2 (or equivalent) <br><br>
If you need to catch up on the math:
<ul>
<li> A brief <a href="http://cs229.stanford.edu/section/cs229-prob.pdf "> probability review</a> from Stanford University;
<li> <a href="http://cs229.stanford.edu/section/cs229-linalg.pdf">Linear algebra and matrix calculus review</a> also from Stanford;</li>
<li> Bishop appendix B,C.</li>
<li> The <a href=https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf> Matrix Cookbook</a> is a collection of matrix identities and relations (very useful for multivariate calculus, e.g. how to take the gradient of a loss w.r.t. a weight vector...)</li>
</ul>
</td></tr>

<tr><td>Sep. 11</td><td>More on Linear Models. Overfitting. Regularization.</td>
<td>

<a href="lectures/lecture-2.pdf">Lecture 2 slides</a> -
Bishop, Sec. 1.3, 3.1, 3.2, Hastie Sec. 3.4, 7.1-3, 7.10<br><br>
If you need to catch up on the math:
<ul>
<li> <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint"> Lagrange multipliers</a> on Khan academy;</li>
<li> Bishop appendix E</li>


</ul>
</td></tr>

<tr><td>Sep. 13</td><td>Bayesian and Probabilistic ML. </td>
<td>
<a href="lectures/lecture-3.pdf">Lecture 3 slides</a> - 
Related lectures and materials <br>
<ul>
<li> Book:Bishop PRML : Section 1.2 (Probability Theory); </li>
<li> Book:Barber BRML : Chapter 1 (Probabilistic Reasoning); </li>
<li> Video  <a href="https://www.youtube.com/watch?v=mgBrXnjF8R4 "> Bayesian Inference</a> from Zoubin Ghahramani;</li>
<li> Book: Bishop PRML: Section 2.3 (The Gaussian Distribution). This is a truly excellent and in-depth discussion! </li>
<li> Book: Bishop PRML: Section 3.3 (Bayesian Linear Regression).</li>
<li> Nando de Freitas has a series of lectures on Bayesian linear regression.</li>
</ul>	
</td></tr>
	      
	      
	      
<tr><td>Sep. 18</td><td>Logistic Regression.</td>
<td>

<a href="lectures/lecture-4.pdf">Lecture 4 slides</a> -
Related material:<br>
<ul>
<li> Bishop, Sec. 4.1.1-4.1.3: Linear models for classification</li>
<li> Bishop, Sec. 4.3.2-4.3.4: Logistic regression </li>
<li> Bishop, Sec. 1.6: Information theory (optional for now but will be relevant for future lectures...)</li>
</ul>

</td></tr>


<tr><td>Sep. 20</td><td>Kernels and Support Vector Machines</td>
<td>
<a href="lectures/lecture-5.pdf">Lecture 5 slides</a> -
Related material:<br>
<ul>
<li> Bishop, Sec. 6.1-6.2: Kernels </li>
<li> Bishop, Sec. 7.1: Support Vector Machines </li>
<li> David Sontag's (NYU) slides on SVMs and kernels: lectures 
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture3.pdf> 3</a>, 
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture4.pdf> 4</a>,
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture5.pdf> 5</a> and 
<a href=http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture6.pdf> 6 </a> 
<li> A python <a href=http://nbviewer.jupyter.org/github/rllabmcgill/rllabmcgill.github.io/blob/master/COMP-652/lectures/SVMs.ipynb>
notebook</a> to experiment with regularization in SVMs</li>
</ul>
</td></tr>



<tr><td>Sep. 25</td><td>Unsupervised Learning and Dimensionality Reduction</td>
<td>
<a href="lectures/lecture-6.pdf">Lecture 6 slides</a> -
Related material:<br>
<ul>
<li>Bishop 12.1, 12.3: PCA and Kernel PCA </li>
<li>Bishop 9.1: <i>k</i>-means</li>
<li><a href="https://www.cs.nyu.edu/~roweis/lle/papers/lleintro.pdf">Locally Linear Embeddings</a> (optional)</li>
</ul>
</td></tr>

	      
	      
<tr><td>Sep. 27</td><td>Non-Parametric Bayesian Models and Gaussian Processes</td>
<td>
<a href="lectures/lecture-7.pdf">Lecture 7 slides</a> -
Related material:<br>
<ul>
<li><a href="http://mlg.eng.cam.ac.uk/zoubin/talks/nips09npb.pdf">Overview of Non-parametric Bayesian Models</a></li>
<li><a href="http://mlss2011.comp.nus.edu.sg/uploads/Site/lect1gp.pdf">Tutorial on Gaussian Processes (why I don't use SVMs)</a></li>
<li><a href="http://videolectures.net/mlss09uk_rasmussen_gp/">Carl Rasmussen's Lecture on Gaussian Processes</a></li>	
<li><a href="http://www.gaussianprocess.org/gpml/">Book on Gaussian Processes</a></li>	
</ul>
</td></tr>

	      
	      
	      
<tr><td>Oct. 2</td><td>Computational Learning Theory</td><td>
<a href="lectures/lecture-8.pdf">Lecture 8 slides</a> - Related material:<br>
<ul>

<li> Olivier Bousquet's <a href=http://ml.typepad.com/Talks/pdf2522.pdf>slides from MLSS 2003 </a> </li>
<li> Alexander Rakhlin's <a href=http://www-stat.wharton.upenn.edu/~rakhlin/ml_summer_school.pdf>slides from MLSS 2012 </a>
<li> Mehryar Mohri's lectures <a href=http://www.cs.nyu.edu/~mohri/mls/>slides</a> at NYU </li>
<li> 
Books to go further: 
<ul>
<li> <a href=http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf>Understanding Machine Learning</a>, Shai Shalev-Shwartz and Shai Ben-David </li>
<li> Foundations of Machine Learning,
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar
</ul>
</li>
</ul>



</td></tr>

<tr><td>Oct. 4</td><td>Probabilistic Graphical Models I</td><td>
<a href="lectures/lecture-9.pdf">Lecture 9 slides</a> 
</td></tr>	      
	   

<tr><td>Oct. 11</td><td>Inference in Graphical Models</td><td>
<a href="lectures/lecture-10.pdf">Lecture 10 slides</a> 
</td></tr>	      
	   

<tr><td>Oct. 16</td><td>Latent Variables Models, Gaussian Mixture Models, Expectation Maximization</td>
<td>
<a href="lectures/lecture-11.pdf">Lecture 11 slides</a> -
Related material:<br>
<ul>
<li> Bishop, Section 9</li>
<li> Hastie, Section 8.5</li>
</ul>
</td></tr>





<tr><td>Oct. 18</td><td>Time series / Inference in Hidden Markov Models</td>
<td>
<a href='lectures/lecture-12.pdf'>Lecture 12 slides</a> 
</td>
</tr>



<tr><td>Oct. 23</td><td>Spectral Learning</td>
          <td><a href=http://www.cs.upc.edu/~bballe/slides/tutorial-emnlp14.pdf>  Lecture 13 slides</a> (pages 1-81) -  courtesy of B. Balle, A. Quattoni and X. Carrreras<br>
<i> Machine Learning </i> <a href=http://www.lancaster.ac.uk/~deballep/papers/preprint-bclq13.pdf>paper</a>
</tr>
	
<tr><td>Oct. 25</td><td>Learning Dynamical Systems: Bayesain Updating, Kalman Filters...</td>
<td>
<a href='lectures/lecture-14.pdf'>Lecture 14 slides</a> 
</td>
</tr>
	

<tr><td>Oct. 30</td><td>Deep Learning : Neural Networks (optional : RNNs, CNNs)</td>
<td>
<a href='lectures/lecture-15.pdf'>Lecture 15 slides</a> 
</td>
</tr>

	
<tr><td>Nov. 1</td><td>Deep Learning II: Bayesian NNs, RNNs, CNNs</td>
<td>
<a href='lectures/lecture-16.pdf'>Lecture 16 slides</a> 
</td>
</tr>


	      
<tr><td>Nov. 6</td><td>Variational Inference</td>
<td>
<a href="lectures/lecture-17.pdf">Lecture 17 slides</a> -
Related material:<br>
<ul>
<li><a href="http://www.cs.columbia.edu/~blei/talks/2016_NIPS_VI_tutorial.pdf">Variational Inference : NIPS 2017 Tutorial - courtesy of David Blei, Rajesh Ranganath, Shakir Mohamed</a></li>
<li><a href="https://arxiv.org/abs/1601.00670">Variational Inference : Review for Statisticians</a></li>
</ul>
</td></tr>	
	

<tr><td>Nov. 8</td><td>Generative Models</td>
<td>
<a href='lectures/lecture-18.pdf'>Lecture 18 slides (Courtesy of Fei Fei-Fei Li, Justin Johnson, Serena Yeung)</a> 
</td>
</tr>
	
	
<tr><td>Nov. 13</td><td>Generative Adversarial Networks</td><td></td></tr>
	
<tr><td>Nov. 15</td><td>Tensor Methods in ML</td><td></td></tr>
<tr><td>Nov. 20</td><td>midterm recap</td><td></td></tr>
<tr><td>Nov. 22</td><td>In-class midtern exam</td><td></td></tr>
<tr><td>Nov. 27</td><td>Reinforcement Learning</td><td></td></tr>
<tr><td>Nov. 29</td><td>More on Reinforcement Learning</td><td></td></tr>
<tr><td>Dec. 4</td><td>TBA</td><td></td></tr>
<tr><td>Dec. 6</td><td>TBA</td><td></td></tr>
<tr><td>Dec. 11</td><td>Projects Presentations 1</td><td></td></tr>
<tr><td>Dec. 13</td><td>Projects Presentations 2</td><td></td></tr>


</table>
<p>
&nbsp;
</p>

  </body>
</html>
